Regularization is a technique in Machine learning to prevent Overfitting, i.e when a model learns the training data too well, Includes noise and work poorly on new data. It does this by adding more bias by adding penalty to models complexity, encouraging simple models that generalize better. That is where the word regularization comes, making regular not complex model.

[[Q1-Why is overfitting needs to be treated separately with Regularization but underfitting is not]]

This is achieved by 2 types of penalties:
1. [[Ridge - L2 Regularization]]
2. [[Lasso - L1 Regularization]]

[[Q2-How are Ridge and Lasso different and where should one be preferred over another]]
3. [[ElasticNet Regularization]]



Questions:
[[Q4-Feature scaling is important for regularized regression because{MCQ}]]